title: 线性回归
author: Rhys Pang
tags:
  - 机器学习
categories:
  - ml
date: 2019-10-06 13:42:00
---
# 线性回归

## 模型
用 $\mathbf{x}=(x_1,x_2,\ldots,x_n)^T$ 表示特征，预测值为这些特征的线性组合
$$
h(\mathbf{x}) = h_{\mathbf{w}}(\mathbf{x}) = w_0 + w_1 x_1 + w_2 x_2 + \ldots + w_n x_n
$$
$h(\mathbf{x})$是关于$\mathbf{w}$的一个线性函数，通过训练数据得到$\mathbf{w}$后，就可以根据上式计算预测值。可以引入$x_0 = 1$，上式可以简化为
$$
h(\mathbf{x}) = h_{\mathbf{w}}(\mathbf{x}) = w_0 x_0+ w_1 x_1 + w_2 x_2 + \ldots + w_n x_n  =  \mathbf{x} \mathbf{w}
$$训练数据为
$$
(\mathbf{x}^1, y^1),(\mathbf{x}^2, y^2),\ldots,(\mathbf{x}^m, y^m)
$$

## 损失函数
使用均方误差来计算模型损失
$$
L(\mathbf{w}) = \frac {1}{m} \sum_{i=1}^m (h_{\mathbf{w}}(\mathbf{x}^i) - y^i)^2
$$
优化目标是求得使$L(\mathbf{w})$最小的$\mathbf{w}$
$$
min_{\mathbf{w}} \frac {1}{m} \sum_{i=1}^m (h_{\mathbf{w}}(\mathbf{x}^i) - y^i)^2
$$
向量表达式为
$$
min_{\mathbf{W}} \frac {1}{m} || \mathbf{X} \mathbf{w} - \mathbf{y} || ^2
$$

## 算法
使用梯度下降对表达式进行优化
$$
w_j := w_j + \alpha \sum_{i=1}^m(y^i - h_{\mathbf w}(\mathbf x^i))x_j^i
$$